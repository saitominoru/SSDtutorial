{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def od_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Datasetから取り出すアノテーションデータのサイズが画像ごとに異なる。\n",
    "    画像内の物体数が２個であれば、(2,5)というサイズになるが、３個であれば(3,5)というサイズになる。\n",
    "    この変化に対応するDataloaderを作成するためにカスタマイズしたcollate_fnを作成する。\n",
    "    collate_fnはPytorchでリストからmini-batchを作成する関数である。\n",
    "    ミニバッチ分の画像が並んでいるリスト変数batchにミニバッチ番号を指定する次元を先頭に１つ追加してリストの形を変形する。\n",
    "    \"\"\"\n",
    "    \n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0]) #sample[0]は画像img\n",
    "        targets.append(torch.FloatTensor(sample[1])) #sample[1]はアノテーションgt\n",
    "        \n",
    "        \n",
    "    #imgsはミニバッチサイズのリストになっている\n",
    "    #リストの要素はtorch.Size([3,300,300])\n",
    "    #このリストをtorch.Size([batch_num, 3, 300, 300])のテンソルに変換する\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    \n",
    "    #*targetsはアノテーションデータの正解であるgtのリスト\n",
    "    #*リストのサイズはミニバッチサイズ\n",
    "    #*リストtargetsの要素は[n, 5]\n",
    "    #*nは画像ごとに異なり、画像内にあるオブジェクトの数になる。\n",
    "    #*5は[xmin,ymin,xmax,ymax,class,index]\n",
    "    return imgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myUtilsData as mUD\n",
    "\n",
    "color_mean = (104, 117, 123) #(BGR)の色の平均値\n",
    "input_size = 300  #画像のサイズを300*300にする\n",
    "\n",
    "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = mUD.make_datapath_list(rootpath)\n",
    "\n",
    "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "train_dataset = mUD.VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform = mUD.DataTransform(input_size, color_mean), transform_anno = mUD.Anno_xml2list(voc_classes))\n",
    "val_dataset = mUD.VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform = mUD.DataTransform(input_size, color_mean), transform_anno = mUD.Anno_xml2list(voc_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvmonitor\n",
      "train\n",
      "person\n",
      "boat\n",
      "cow\n",
      "cow\n",
      "torch.Size([4, 3, 300, 300])\n",
      "4\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "batch_size=4\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=mUD.od_collate_fn)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=mUD.od_collate_fn)\n",
    "\n",
    "#辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "\n",
    "#動作確認\n",
    "batch_iterator = iter(dataloaders_dict[\"val\"]) #イテレータに変換\n",
    "images, targets = next(batch_iterator) #1番目の要素を取り出す\n",
    "print(images.size())  #torch.Size([4,3,300,300])\n",
    "print(len(targets))\n",
    "print(targets[1].size()) #ミニバッチのサイズのリスト、各要素は[n,5]、nは物体数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
